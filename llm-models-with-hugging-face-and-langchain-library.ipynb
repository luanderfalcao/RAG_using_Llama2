{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/givkashi/huggingface-llm-langchain/blob/main/llm-models-with-hugging-face-and-langchain-library.ipynb?source=post_page-----4994e7ed5c06--------------------------------#scrollTo=Rj6S_sUQ9o6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from langchain import PromptTemplate, HuggingFacePipeline\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.messages import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"]='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# MODEL_NAME =\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# MODEL_NAME =\"meta-llama/Meta-Llama-3-8B\"\n",
    "MODEL_NAME =\"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# MODEL_NAME =\"microsoft/phi-1_5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization is a technique used to reduce the memory and computation requirements\n",
    "# of deep learning models, typically by using fewer bits, 4 bits\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of a tokenizer for the language model,\n",
    "# necessary to preprocess text data for input\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28579e8d7f942189c7688e9d64b94bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialization of the pre-trained language model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    "    #quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of some generation-related settings\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 256 # maximum number of new tokens that can be generated by the model\n",
    "generation_config.temperature = 0.2 # randomness of the generated tex\n",
    "generation_config.top_p = 0 # diversity of the generated text\n",
    "generation_config.do_sample = True # sampling during the generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pipeline is an object that works as an API for calling the model\n",
    "# The pipeline is made of (1) the tokenizer instance, the model instance, and\n",
    "# some post-procesing settings. Here, it's configured to return full-text outputs\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace pipeline\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_text = \"What the city has a red bridge at california?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What the city has a red bridge at california?\n",
      "\n",
      "# Answer\n",
      "The city with a red bridge in California is San Francisco. The iconic red bridge you're referring to is the Golden Gate Bridge, which is one of the most recognized symbols of San Francisco and California. The bridge spans the Golden Gate, the one-mile-wide strait connecting San Francisco Bay and the Pacific Ocean. The color of the bridge, officially known as \"International Orange,\" was chosen to enhance its visibility in the fog and to complement the natural surroundings. The Golden Gate Bridge was completed in 1937 and has since become an enduring symbol of the city and a popular tourist attraction.\n"
     ]
    }
   ],
   "source": [
    "output = llm.invoke(input_text)\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
